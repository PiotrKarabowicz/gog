{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                            Text analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9575f3529bee41e58dcd5a94e794d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='write disease here...', description='Disease:   ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac74902a9bd246489b16724e6b2074c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1000, description='No. abstracts:', max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as wg\n",
    "from IPython.display import display\n",
    "\n",
    "myName = wg.Text(value='write disease here...', description='Disease:   ')\n",
    "N_abstract = wg.IntSlider(description='No. abstracts:', max=10000, value= 1000)\n",
    "display(myName,N_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/piotr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import ipywidgets as wg\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected disease: liver cancer\n",
      "Number of abstracts:  1000\n"
     ]
    }
   ],
   "source": [
    "print('Selected disease: ' + myName.value) \n",
    "print('Number of abstracts:  ' + str(N_abstract.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "<ipython-input-21-52140cebb358>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['class'] = result2 # dataframe z wynikami\n",
      "<ipython-input-21-52140cebb358>:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['abstracts_stop'] = df_abstract_1['abstracts_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "<ipython-input-21-52140cebb358>:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['tokenized'] = df_abstract_1.apply(lambda row: nltk.word_tokenize(row['abstracts_lower']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    text = myName.value\n",
    "    max1 =int(N_abstract.value)  # ilosc zapytan\n",
    "    #text = [text]\n",
    "\n",
    "    #wporwadzenie zapytania do pubmed\n",
    "    pubmed = PubMed(tool=\"MyTool\", email=\"p.karabowicz@gmail.com\")\n",
    "    results1 = pubmed.query(text, max_results=max1)\n",
    "\n",
    "    #przeksztalcenie wynikow zapytania na data frame\n",
    "    lista_abstract_3=[]\n",
    "    for i in results1:\n",
    "        lista_abstract_3.append(i.abstract)\n",
    "    import pandas as pd\n",
    "    df_abstract = pd.DataFrame(lista_abstract_3, columns = ['abstracts'])\n",
    "    df_abstract['abstracts_lower'] = df_abstract['abstracts'].str.lower()\n",
    "    df_abstract_1 = df_abstract.dropna() #datafraame wynikow do analizy\n",
    "\n",
    "    ###predykcja\n",
    "    #from keras.models import load_model\n",
    "    #BASE_DIR = os.path.dirname(os.path.abspath('finalized_model1.sav'))\n",
    "    #BASE_DIR1 = '/home/piotr/drugfinal/static'\n",
    "    #finalized_model= load_model(os.path.join(BASE_DIR, 'static/finalized_model1.sav'))\n",
    "    \n",
    "    #finalized_model= os.path.join(BASE_DIR, 'static/finalized_model1.sav')\n",
    "    import pickle\n",
    "    rnd = pickle.load(open('./finalized_model.sav', 'rb'))\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100)\n",
    "    result1=count_vect.fit_transform(df_abstract_1['abstracts_lower'])\n",
    "    result2 = rnd.predict(result1)\n",
    "    df_abstract_1['class'] = result2 # dataframe z wynikami\n",
    "    len_df = len(result2)\n",
    "\n",
    "#unsupervised learning\n",
    "    from gensim.models import Word2Vec\n",
    "    from nltk.corpus import stopwords\n",
    "    #przygotowanie tekstu do osadzania slow\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    df_abstract_1['abstracts_stop'] = df_abstract_1['abstracts_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df_abstract_1['tokenized'] = df_abstract_1.apply(lambda row: nltk.word_tokenize(row['abstracts_lower']), axis=1)\n",
    "    model_ted1 = Word2Vec(sentences=df_abstract_1['tokenized'], size=200, window=10, min_count=1, workers=4, sg=0)\n",
    "\n",
    "    #ekstrackja slow najbardziej podobnych do protein i target\n",
    "    keys = ['protein', 'target']\n",
    "    most_sim = model_ted1.wv.most_similar(positive = keys, topn=1000)\n",
    "    #utworzenie tablicy ze slownika\n",
    "    most_sim_key = []\n",
    "    for w, n in most_sim:\n",
    "        most_sim_key.append(w)\n",
    "    #tagowanie tekstu i filtrowanie wedlug tagow\n",
    "    post_tag_list = nltk.pos_tag(most_sim_key)\n",
    "\n",
    "    listNN = []\n",
    "    for w , k in post_tag_list:\n",
    "        if k == \"NN\":\n",
    "            listNN.append(w)\n",
    "\n",
    "    listJJ = []\n",
    "    for w , k in post_tag_list:\n",
    "        if k == \"JJ\":\n",
    "            listJJ.append(w)\n",
    "\n",
    "    lista = [listNN, listJJ]\n",
    "\n",
    "    flat_list = []\n",
    "    for sublist in lista:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    #filtrowanie wzgledem bazy bialek\n",
    "    #BASE_DIR2 = os.path.dirname(os.path.abspath('lista_bialek_bez1.txt'))\n",
    "    file = open('./lista_bialek_bez1.txt', \"r\")\n",
    "    wprowadzony_tekst = file.read()\n",
    "\n",
    "    wt = wprowadzony_tekst.split(',')\n",
    "\n",
    "    wt2 =[]\n",
    "    for w in wt:\n",
    "        w = w.replace(\"[\", \"\")\n",
    "        w = w.replace(\"]\", \"\")\n",
    "        w= w.replace(\"'\", \"\")\n",
    "        wt2.append(w)\n",
    "\n",
    "    wt2 = [x.lower() for x in wt2 ]\n",
    "    tablica_in =[] #lista wyekstrahowanych bialek\n",
    "    for w in flat_list:\n",
    "        if w in wt2:\n",
    "            tablica_in.append(w)\n",
    "\n",
    "    # predykcja dla zapytan z tablica_in\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import time\n",
    "\n",
    "    def query(list_target):\n",
    "        pubmed = PubMed(tool=\"MyTool\", email=\"p.karabowicz@gmail.com\")\n",
    "        lista=[]\n",
    "        for w in list_target:\n",
    "            time.sleep(1)\n",
    "            lista.append(pubmed.query(w, max_results=20))\n",
    "        return lista\n",
    "\n",
    "    def lista_bastract_pred1(lista):\n",
    "        lista_abstract_pred=[]\n",
    "        for n in lista:\n",
    "                lista_abstract_pred.append(n.abstract)\n",
    "        return lista_abstract_pred\n",
    "\n",
    "    def percent_true(ynew2):\n",
    "        percent_true = round((list(ynew2).count(1))/len(list(ynew2))*100,1)\n",
    "        return percent_true\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    import pickle\n",
    "    #rnd = pickle.load(open('C:\\\\Users\\\\UMB\\\\Desktop\\\\drugforest\\\\drugforest\\\\static\\\\finalized_model_32.sav', 'rb'))\n",
    "\n",
    "    a = query(tablica_in)\n",
    "    d = []\n",
    "    for w in a:\n",
    "        b = lista_bastract_pred1(w) ## ziterowaÄ‡\n",
    "        d.append(b)\n",
    "\n",
    "    d2=[]\n",
    "    for g in d:\n",
    "        d1 = list(filter(None.__ne__, g))\n",
    "        d2.append(d1)\n",
    "\n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100)\n",
    "    lista_y=[]\n",
    "    for w in d2:\n",
    "        result1=count_vect.fit_transform(w)\n",
    "        result2 = rnd.predict(result1)\n",
    "        lista_y.append(result2)\n",
    "\n",
    "    yy = []\n",
    "    for y in lista_y:\n",
    "        yy.append(percent_true(y))\n",
    "\n",
    "    import pandas as pd\n",
    "    data_tuples = list(zip(tablica_in,yy))\n",
    "    df_list = pd.DataFrame(data_tuples, columns=['Protein','Score'])\n",
    "\n",
    "    df_list = df_list.sort_values('Score', ascending = False)\n",
    "    #df_list = df_list.reset_index()\n",
    "\n",
    "    df_abstract_1 = df_abstract_1[['abstracts', 'class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pp2a</td>\n",
       "      <td>78.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vdr</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>emt</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mcs</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hep</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>next</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mec</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ccr2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lkb1</td>\n",
       "      <td>63.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e-cadherin</td>\n",
       "      <td>63.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mtor</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aid</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>acdase</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ir</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>il-22</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mir</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Protein  Score\n",
       "8         pp2a   78.9\n",
       "5          vdr   75.0\n",
       "14         emt   75.0\n",
       "4          mcs   70.0\n",
       "7          hep   70.0\n",
       "11        next   68.4\n",
       "6          mec   65.0\n",
       "9         ccr2   65.0\n",
       "15        lkb1   63.2\n",
       "10  e-cadherin   63.2\n",
       "3         mtor   60.0\n",
       "1          aid   60.0\n",
       "12      acdase   55.0\n",
       "2           ir   50.0\n",
       "13       il-22   50.0\n",
       "16         mir   45.0\n",
       "0           gs   40.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[['Protein', 'Score']]\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
