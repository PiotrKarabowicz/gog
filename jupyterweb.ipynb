{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "#!pip install pymed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/piotr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/piotr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymed import PubMed\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import ipywidgets as wg\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168f4c1f64fa4f1887d6e1ae8085aeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='disease')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a5bfe472264c4992f3222fe332a6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='No. abstracts:', max=10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myName = wg.Text(value='disease')\n",
    "N_abstract = wg.IntSlider(description='No. abstracts:', max=10000)\n",
    "display(myName,N_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liver cancer is 6774 years old\n"
     ]
    }
   ],
   "source": [
    "print(myName.value + ' is ' + str(N_abstract.value) + ' years old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/home/piotr/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "<ipython-input-5-41c84311faf1>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['class'] = result2 # dataframe z wynikami\n",
      "<ipython-input-5-41c84311faf1>:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['abstracts_stop'] = df_abstract_1['abstracts_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "<ipython-input-5-41c84311faf1>:221: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_abstract_1['tokenized'] = df_abstract_1.apply(lambda row: nltk.word_tokenize(row['abstracts_lower']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    text = myName.value\n",
    "    max1 =int(N_abstract.value)  # ilosc zapytan\n",
    "    #text = [text]\n",
    "\n",
    "    #wporwadzenie zapytania do pubmed\n",
    "    pubmed = PubMed(tool=\"MyTool\", email=\"p.karabowicz@gmail.com\")\n",
    "    results1 = pubmed.query(text, max_results=max1)\n",
    "\n",
    "    #przeksztalcenie wynikow zapytania na data frame\n",
    "    lista_abstract_3=[]\n",
    "    for i in results1:\n",
    "        lista_abstract_3.append(i.abstract)\n",
    "    import pandas as pd\n",
    "    df_abstract = pd.DataFrame(lista_abstract_3, columns = ['abstracts'])\n",
    "    df_abstract['abstracts_lower'] = df_abstract['abstracts'].str.lower()\n",
    "    df_abstract_1 = df_abstract.dropna() #datafraame wynikow do analizy\n",
    "\n",
    "    ###predykcja\n",
    "    #from keras.models import load_model\n",
    "    #BASE_DIR = os.path.dirname(os.path.abspath('finalized_model1.sav'))\n",
    "    #BASE_DIR1 = '/home/piotr/drugfinal/static'\n",
    "    #finalized_model= load_model(os.path.join(BASE_DIR, 'static/finalized_model1.sav'))\n",
    "    \n",
    "    #finalized_model= os.path.join(BASE_DIR, 'static/finalized_model1.sav')\n",
    "    import pickle\n",
    "    rnd = pickle.load(open('./finalized_model.sav', 'rb'))\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100)\n",
    "    result1=count_vect.fit_transform(df_abstract_1['abstracts_lower'])\n",
    "    result2 = rnd.predict(result1)\n",
    "    df_abstract_1['class'] = result2 # dataframe z wynikami\n",
    "    len_df = len(result2)\n",
    "\n",
    "#unsupervised learning\n",
    "    from gensim.models import Word2Vec\n",
    "    #from nltk.corpus import stopwords\n",
    "    #przygotowanie tekstu do osadzania slow\n",
    "    #stop = stopwords.words('english')\n",
    "    stop = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    df_abstract_1['abstracts_stop'] = df_abstract_1['abstracts_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    df_abstract_1['tokenized'] = df_abstract_1.apply(lambda row: nltk.word_tokenize(row['abstracts_lower']), axis=1)\n",
    "    model_ted1 = Word2Vec(sentences=df_abstract_1['tokenized'], size=200, window=10, min_count=1, workers=4, sg=0)\n",
    "\n",
    "    #ekstrackja slow najbardziej podobnych do protein i target\n",
    "    keys = ['protein', 'target']\n",
    "    most_sim = model_ted1.wv.most_similar(positive = keys, topn=1000)\n",
    "    #utworzenie tablicy ze slownika\n",
    "    most_sim_key = []\n",
    "    for w, n in most_sim:\n",
    "        most_sim_key.append(w)\n",
    "    #tagowanie tekstu i filtrowanie wedlug tagow\n",
    "    post_tag_list = nltk.pos_tag(most_sim_key)\n",
    "\n",
    "    listNN = []\n",
    "    for w , k in post_tag_list:\n",
    "        if k == \"NN\":\n",
    "            listNN.append(w)\n",
    "\n",
    "    listJJ = []\n",
    "    for w , k in post_tag_list:\n",
    "        if k == \"JJ\":\n",
    "            listJJ.append(w)\n",
    "\n",
    "    lista = [listNN, listJJ]\n",
    "\n",
    "    flat_list = []\n",
    "    for sublist in lista:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    #filtrowanie wzgledem bazy bialek\n",
    "    #BASE_DIR2 = os.path.dirname(os.path.abspath('lista_bialek_bez1.txt'))\n",
    "    file = open('./lista_bialek_bez1.txt', \"r\")\n",
    "    wprowadzony_tekst = file.read()\n",
    "\n",
    "    wt = wprowadzony_tekst.split(',')\n",
    "\n",
    "    wt2 =[]\n",
    "    for w in wt:\n",
    "        w = w.replace(\"[\", \"\")\n",
    "        w = w.replace(\"]\", \"\")\n",
    "        w= w.replace(\"'\", \"\")\n",
    "        wt2.append(w)\n",
    "\n",
    "    wt2 = [x.lower() for x in wt2 ]\n",
    "    tablica_in =[] #lista wyekstrahowanych bialek\n",
    "    for w in flat_list:\n",
    "        if w in wt2:\n",
    "            tablica_in.append(w)\n",
    "\n",
    "    # predykcja dla zapytan z tablica_in\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import time\n",
    "\n",
    "    def query(list_target):\n",
    "        pubmed = PubMed(tool=\"MyTool\", email=\"p.karabowicz@gmail.com\")\n",
    "        lista=[]\n",
    "        for w in list_target:\n",
    "            time.sleep(1)\n",
    "            lista.append(pubmed.query(w, max_results=20))\n",
    "        return lista\n",
    "\n",
    "    def lista_bastract_pred1(lista):\n",
    "        lista_abstract_pred=[]\n",
    "        for n in lista:\n",
    "                lista_abstract_pred.append(n.abstract)\n",
    "        return lista_abstract_pred\n",
    "\n",
    "    def percent_true(ynew2):\n",
    "        percent_true = round((list(ynew2).count(1))/len(list(ynew2))*100,1)\n",
    "        return percent_true\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    import pickle\n",
    "    #rnd = pickle.load(open('C:\\\\Users\\\\UMB\\\\Desktop\\\\drugforest\\\\drugforest\\\\static\\\\finalized_model_32.sav', 'rb'))\n",
    "\n",
    "    a = query(tablica_in)\n",
    "    d = []\n",
    "    for w in a:\n",
    "        b = lista_bastract_pred1(w) ## ziterować\n",
    "        d.append(b)\n",
    "\n",
    "    d2=[]\n",
    "    for g in d:\n",
    "        d1 = list(filter(None.__ne__, g))\n",
    "        d2.append(d1)\n",
    "\n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100)\n",
    "    lista_y=[]\n",
    "    for w in d2:\n",
    "        result1=count_vect.fit_transform(w)\n",
    "        result2 = rnd.predict(result1)\n",
    "        lista_y.append(result2)\n",
    "\n",
    "    yy = []\n",
    "    for y in lista_y:\n",
    "        yy.append(percent_true(y))\n",
    "\n",
    "    import pandas as pd\n",
    "    data_tuples = list(zip(tablica_in,yy))\n",
    "    df_list = pd.DataFrame(data_tuples, columns=['Protein','Score'])\n",
    "\n",
    "    df_list = df_list.sort_values('Score', ascending = False)\n",
    "    #df_list = df_list.reset_index()\n",
    "\n",
    "    df_abstract_1 = df_abstract_1[['abstracts', 'class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>protease</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>apoa-i</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dnmt1</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lkb1</td>\n",
       "      <td>73.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>siglec-15</td>\n",
       "      <td>70.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p38</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>egf</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cyclin</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bfgf</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>act</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mir</td>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>il-10</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>rig-i</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rnd3</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p21</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mmp-9</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>icam-1</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>st2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>e-cadherin</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cox-2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>smad2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>igfbp-3</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cer</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cdk1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mtor</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pcna</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emt</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>il-6</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rb</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>hif-1</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p65</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>p22</td>\n",
       "      <td>42.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sp1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nfatc2</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smad3</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>chrebp</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hk2</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ve-cadherin</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>beta</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gr</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Protein  Score\n",
       "17     protease   75.0\n",
       "34       apoa-i   75.0\n",
       "15        dnmt1   75.0\n",
       "19         lkb1   73.7\n",
       "31    siglec-15   70.6\n",
       "2           p38   70.0\n",
       "38          egf   70.0\n",
       "1        cyclin   70.0\n",
       "18         bfgf   70.0\n",
       "12          act   68.4\n",
       "3           mir   68.4\n",
       "37        il-10   65.0\n",
       "35        rig-i   65.0\n",
       "7          rnd3   65.0\n",
       "11          p21   65.0\n",
       "22        mmp-9   60.0\n",
       "20       icam-1   60.0\n",
       "28          st2   60.0\n",
       "24   e-cadherin   60.0\n",
       "32        cox-2   60.0\n",
       "23        smad2   60.0\n",
       "29      igfbp-3   55.0\n",
       "8           cer   55.0\n",
       "27         cdk1   50.0\n",
       "0          mtor   50.0\n",
       "13         pcna   50.0\n",
       "5           emt   50.0\n",
       "26         il-6   45.0\n",
       "10           rb   45.0\n",
       "33        hif-1   45.0\n",
       "9           p65   45.0\n",
       "16          p22   42.1\n",
       "25          sp1   40.0\n",
       "21       nfatc2   40.0\n",
       "14        smad3   40.0\n",
       "30       chrebp   40.0\n",
       "6           hk2   35.0\n",
       "36  ve-cadherin   35.0\n",
       "39         beta   35.0\n",
       "4            gr   30.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[['Protein', 'Score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liver cancer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myName.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1aff9af0cee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpubmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'liver cancer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlista_abstract_11\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlista_abstract_11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pymed/api.py\u001b[0m in \u001b[0;36m_getArticles\u001b[0;34m(self, article_ids)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Parse as XML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# Loop over the articles and construct article objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mXML\u001b[0;34m(text, parser)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXMLParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTreeBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#results11 = pubmed.query('liver cancer', max_results=10000)\n",
    "#lista_abstract_11=[]\n",
    "#for i in results11:\n",
    "    #lista_abstract_11.append(i.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
